{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/lakehouse/default/Files/*.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sheets(filename):\n",
    "    workbook = pd.read_excel(filename, sheet_name=None)\n",
    "\n",
    "    df_all_sheets = pd.DataFrame()\n",
    "\n",
    "    for sheet_name, sheet in workbook.items():\n",
    "\n",
    "        df_sheet = pd.melt(\n",
    "            sheet,\n",
    "            id_vars=[\"ProductKey\"],\n",
    "            var_name=\"Date\",\n",
    "            value_name=\"Qty\"\n",
    "        )\n",
    "\n",
    "        df_sheet.dropna(subset=[\"Qty\"], inplace=True)\n",
    "        df_all_sheets = pd.concat([df_all_sheets, df_sheet], ignore_index=True)\n",
    "\n",
    "    return(df_all_sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(files):\n",
    "    df_all_files = pd.DataFrame()\n",
    "\n",
    "    for filename in files:\n",
    "        file = combine_sheets(filename)\n",
    "        df_all_files = pd.concat([df_all_files, file], ignore_index=True) \\\n",
    "            .sort_values(by=[\"Date\"])\n",
    "\n",
    "    return(df_all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_files = combine_files(files)\n",
    "print(df_all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"ProductKey\", IntegerType(), True) \\\n",
    "    .add(\"Date\", DateType(), True) \\\n",
    "    .add(\"Qty\", IntegerType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.createDataFrame(df_all_files, schema=schema)\n",
    "display(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"tb_combined\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
