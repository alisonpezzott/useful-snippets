{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc127fb3-6d2f-4483-b17d-b61f413945ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 0: Configure Spark to treat table and column names as case-sensitive\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "\n",
    "# Step 1: Define SharePoint site and document library details\n",
    "TENANT_HOST         = \"overdax.sharepoint.com\"  # SharePoint Host\n",
    "SITE_PATH           = \"Sandbox\"                 # Site path\n",
    "DOCS_DRIVE          = \"Documents\"               # Documents folder path name\n",
    "ROOT_FOLDER         = None                      # Optional start subfolder\n",
    "TARGET_LANDING_PATH = \"Files/Landing\"           # Lakehouse landing path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b275d-8e2e-4301-927d-8f43cf6dec5b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Retrieve secrets from Azure Key Vault\n",
    "KEY_VAULT     = \"https://pezzott.vault.azure.net/\"\n",
    "CLIENT_ID     = notebookutils.credentials.getSecret(KEY_VAULT, \"graph-client-id\")\n",
    "TENANT_ID     = notebookutils.credentials.getSecret(KEY_VAULT, \"graph-tenant-id\")\n",
    "CLIENT_SECRET = notebookutils.credentials.getSecret(KEY_VAULT, \"graph-secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa2911-9502-4174-b38e-0158e968a3bc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Acquire access token for Microsoft Graph API\n",
    "import requests\n",
    "\n",
    "token_url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
    "token_resp = requests.post(\n",
    "    token_url,\n",
    "    data={\n",
    "        \"client_id\": CLIENT_ID,\n",
    "        \"client_secret\": CLIENT_SECRET,\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"scope\": \"https://graph.microsoft.com/.default\",  # Graph app perms\n",
    "    },\n",
    ")\n",
    "token_resp.raise_for_status()\n",
    "access_token = token_resp.json()[\"access_token\"]\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\", \"Accept\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011fcf7-f45a-4876-9454-e2f2bf73e9b3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Locate SharePoint document library by name\n",
    "site = requests.get(\n",
    "    f\"https://graph.microsoft.com/v1.0/sites/{TENANT_HOST}:{SITE_PATH}\",\n",
    "    headers=headers\n",
    ").json()\n",
    "site_id = site[\"id\"]\n",
    "\n",
    "drives = requests.get(f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives?$select=id,name,driveType\", headers=headers).json()\n",
    "drive_id = next(d[\"id\"] for d in drives.get(\"value\", []) if d[\"name\"] == DOCS_DRIVE)\n",
    "print(f\"Drive ID: {drive_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335cd1c-9039-425a-95ce-0c1c356c318d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: List all files in the SharePoint document library (recursive traversal)\n",
    "def list_children(drive_id, item_id):\n",
    "    url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{item_id}/children?$top=200\"\n",
    "    while url:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        for it in data.get(\"value\", []):\n",
    "            yield it\n",
    "        url = data.get(\"@odata.nextLink\")\n",
    "\n",
    "# Resolve the root of the SharePoint drive\n",
    "root = requests.get(f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root\", headers=headers).json()\n",
    "root_id = root[\"id\"]\n",
    "\n",
    "# If a root folder is specified, resolve it first\n",
    "if ROOT_FOLDER:\n",
    "    path = f\"/{ROOT_FOLDER}\".strip(\"/\")\n",
    "    folder_item = requests.get(\n",
    "        f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:/{quote(path)}\",\n",
    "        headers=headers\n",
    "    ).json()\n",
    "    base_id = folder_item[\"id\"]\n",
    "else:\n",
    "    base_id = root_id\n",
    "\n",
    "# Traverse all folders and collect every file (no file extension filter)\n",
    "all_items = []\n",
    "stack = [base_id]\n",
    "while stack:\n",
    "    current = stack.pop()\n",
    "    for it in list_children(drive_id, current):\n",
    "        if \"folder\" in it:\n",
    "            stack.append(it[\"id\"])\n",
    "        elif \"file\" in it:\n",
    "            all_items.append(it)\n",
    "\n",
    "print(f\"Found {len(all_items)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3039f4-7a12-4a58-b41d-b94cfd6a823d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions for Lakehouse operations\n",
    "import os, uuid, pathlib\n",
    "\n",
    "def ensure_dir(dir_path: str):\n",
    "    if dir_path and not notebookutils.fs.exists(dir_path):\n",
    "        notebookutils.fs.mkdirs(dir_path)\n",
    "\n",
    "def save_binary_to_lakehouse(target_rel_path: str, content: bytes, overwrite: bool = True):\n",
    "    \"\"\"\n",
    "    Save files to lakehouse coping from a temp path.\n",
    "    \"\"\"\n",
    "    tmp_dir = \"/tmp\"\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    tmp_path = os.path.join(tmp_dir, f\"{uuid.uuid4()}_{pathlib.Path(target_rel_path).name}\")\n",
    "    with open(tmp_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    parent = \"/\".join(target_rel_path.split(\"/\")[:-1])\n",
    "    ensure_dir(parent)\n",
    "\n",
    "    if overwrite and notebookutils.fs.exists(target_rel_path):\n",
    "        notebookutils.fs.rm(target_rel_path)\n",
    "\n",
    "    notebookutils.fs.cp(f\"file:{tmp_path}\", target_rel_path)\n",
    "\n",
    "    try:\n",
    "        os.remove(tmp_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def relative_parent_from_item(item, root_folder: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Convert parentReference.path to short relative path.\n",
    "    Removes any prefix '.../root:' (ex.: '/drive/root:' OU '/drives/<id>/root:')\n",
    "    and, if exists, removes the ROOT_FOLDER from the start too.\n",
    "    \"\"\"\n",
    "    p = item.get(\"parentReference\", {}).get(\"path\", \"\")\n",
    "    # Possible examples: \n",
    "    # '/drive/root:/Test/Sub'\n",
    "    # '/drives/b!abc123/root:/Test/Sub'\n",
    "    # '/drives/b!abc123/root:'  (arquivo diretamente na raiz)\n",
    "\n",
    "    if \"root:\" in p:\n",
    "        # Take all after 'root:'\n",
    "        p = p.split(\"root:\", 1)[1]   # -> ': /Test/Sub' or '' (if root)\n",
    "\n",
    "    # Clean initial ':' and slashes\n",
    "    p = p.lstrip(\":\").lstrip(\"/\")    # -> 'Test/Sub' or ''\n",
    "\n",
    "    # Remove ROOT_FOLDER from start, if defined\n",
    "    if root_folder:\n",
    "        rf = root_folder.strip(\"/\")\n",
    "        # Case-insensitive comparision\n",
    "        if p.lower() == rf.lower():\n",
    "            p = \"\"\n",
    "        elif p.lower().startswith(rf.lower() + \"/\"):\n",
    "            p = p[len(rf) + 1 :]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22a8f4-a2c5-4fc1-a2be-4cf58d5acf3f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Download files from SharePoint and land them into the Lakehouse\n",
    "landed_paths = []\n",
    "for item in all_items:\n",
    "    # Try downloadUrl (pre-assigned); else use content\n",
    "    download_url = item.get(\"@microsoft.graph.downloadUrl\")\n",
    "    if download_url:\n",
    "        resp = requests.get(download_url)\n",
    "        resp.raise_for_status()\n",
    "        content = resp.content\n",
    "    else:\n",
    "        content_url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/items/{item['id']}/content\"\n",
    "        resp = requests.get(content_url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        content = resp.content\n",
    "\n",
    "    # Short path on Files/Landing (without no drives/<id>/root:)\n",
    "    relative_parent = relative_parent_from_item(item, ROOT_FOLDER)\n",
    "    if relative_parent:\n",
    "        rel = f\"{TARGET_LANDING_PATH}/{relative_parent}/{item['name']}\"\n",
    "    else:\n",
    "        rel = f\"{TARGET_LANDING_PATH}/{item['name']}\"\n",
    "    save_binary_to_lakehouse(rel, content, overwrite=True)\n",
    "    landed_paths.append(rel)\n",
    "\n",
    "print(f\"Landed {len(landed_paths)} files into Lakehouse.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "012ff4ce-d64f-484b-a041-9b9fe20c98fc",
    "default_lakehouse_name": "LK_SharePoint",
    "default_lakehouse_workspace_id": "0d4a2f53-8c7b-42b0-8500-da59d8da80df",
    "known_lakehouses": [
     {
      "id": "012ff4ce-d64f-484b-a041-9b9fe20c98fc"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
