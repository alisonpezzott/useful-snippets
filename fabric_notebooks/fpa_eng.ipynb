{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "spark.conf.set('spark.sql.caseSensitive', True)\n",
                "from pyspark.sql.functions import lpad, col, expr, round as spark_round\n",
                "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType, LongType\n",
                "import requests, json\n",
                "from datetime import datetime\n",
                "from notebookutils import mssparkutils"
            ],
            "outputs": [],
            "execution_count": 1,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":\"2025-06-05T23:44:16.0629129Z\",\"execution_start_time\":\"2025-06-05T23:44:31.4852287Z\",\"execution_finish_time\":\"2025-06-05T23:44:31.8658155Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "5a5cfa51-907c-46a0-91ed-0920b1fe39ab"
        },
        {
            "cell_type": "code",
            "source": [
                "workspace_name = \"FPA_ENG\"\n",
                "lake_abfss = f\"abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/Lakehouse.Lakehouse\"\n",
                "files_path = lake_abfss + \"/Files/\"\n",
                "tables_path = lake_abfss + \"/Tables/\"\n",
                "\n",
                "blob_path = \"https://raw.githubusercontent.com/alisonpezzott/budget-versus-actuals-sample-data/refs/heads/main/\"\n"
            ],
            "outputs": [],
            "execution_count": 2,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-05T23:46:56.5588205Z\",\"execution_finish_time\":\"2025-06-05T23:46:56.8712153Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "17651a8e-3bad-4cfb-ab4a-8da18c65375f"
        },
        {
            "cell_type": "code",
            "source": [
                "# DimAccount\n",
                "table_name = \"DimAccount\"\n",
                "url = blob_path + table_name + \".csv\"\n",
                "response = requests.get(url)\n",
                "\n",
                "local_path = f\"/tmp/{table_name}.csv\"\n",
                "with open(local_path, 'wb') as f:\n",
                "    f.write(response.content)\n",
                "\n",
                "mssparkutils.fs.cp(f\"file:{local_path}\", files_path + table_name + \".csv\")\n",
                "\n",
                "schema = StructType() \\\n",
                "    .add(\"AccountGroupKey\", StringType(), True) \\\n",
                "    .add(\"AccountSubgroupKey\", StringType(), True) \\\n",
                "    .add(\"AccountSubgroup\", StringType(), True) \\\n",
                "    .add(\"ControlAccountKey\", StringType(), True) \\\n",
                "    .add(\"ControlAccount\", StringType(), True) \\\n",
                "    .add(\"SubsidiaryAccountKey\", StringType(), True) \\\n",
                "    .add(\"SubsidiaryAccount\", StringType(), True)\n",
                "\n",
                "df = spark.read.format(\"csv\") \\\n",
                "    .option(\"header\",\"true\") \\\n",
                "    .schema(schema) \\\n",
                "    .load(files_path + table_name + \".csv\") \\\n",
                "    .withColumn(\"AccountGroupKey\", lpad(col(\"AccountGroupKey\"), 2, \"0\")) \\\n",
                "    .withColumn(\"AccountSubgroupKey\", lpad(col(\"AccountSubgroupKey\"), 5, \"0\")) \\\n",
                "    .withColumn(\"ControlAccountKey\", lpad(col(\"ControlAccountKey\"), 8, \"0\")) \\\n",
                "    .withColumn(\"SubsidiaryAccountKey\", lpad(col(\"SubsidiaryAccountKey\"), 11, \"0\"))\n",
                "\n",
                "\n",
                "df.write.format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .save(tables_path + table_name)\n"
            ],
            "outputs": [],
            "execution_count": 11,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-05T23:30:59.5863321Z\",\"execution_finish_time\":\"2025-06-05T23:31:03.0843532Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "6903fe29-6726-42d5-bd12-d3d6967edbe9"
        },
        {
            "cell_type": "code",
            "source": [
                "# DimAccountGroup\n",
                "table_name = \"DimAccountGroup\"\n",
                "url = blob_path + table_name + \".csv\"\n",
                "response = requests.get(url)\n",
                "\n",
                "local_path = f\"/tmp/{table_name}.csv\"\n",
                "with open(local_path, 'wb') as f:\n",
                "    f.write(response.content)\n",
                "\n",
                "mssparkutils.fs.cp(f\"file:{local_path}\", files_path + table_name + \".csv\")\n",
                "\n",
                "schema = StructType() \\\n",
                "    .add(\"AccountGroup\", StringType(), True) \\\n",
                "    .add(\"AccountGroupKey\", StringType(), True) \\\n",
                "    .add(\"isSubtotal\", StringType(), True)\n",
                "\n",
                "df = spark.read.format(\"csv\") \\\n",
                "    .option(\"header\",\"true\") \\\n",
                "    .schema(schema) \\\n",
                "    .load(files_path + table_name + \".csv\") \\\n",
                "    .withColumn(\"AccountGroupKey\", lpad(col(\"AccountGroupKey\"), 2, \"0\")) \n",
                "\n",
                "df.write.format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .save(tables_path + table_name)\n",
                "    "
            ],
            "outputs": [],
            "execution_count": 4,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-05T23:24:09.2614031Z\",\"execution_finish_time\":\"2025-06-05T23:24:13.9527452Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "b1d26499-7930-41bc-b0c5-e2769eaad85f"
        },
        {
            "cell_type": "code",
            "source": [
                "# DimBranch\n",
                "table_name = \"DimBranch\"\n",
                "url = blob_path + table_name + \".csv\"\n",
                "response = requests.get(url)\n",
                "\n",
                "local_path = f\"/tmp/{table_name}.csv\"\n",
                "with open(local_path, 'wb') as f:\n",
                "    f.write(response.content)\n",
                "\n",
                "mssparkutils.fs.cp(f\"file:{local_path}\", files_path + table_name + \".csv\")\n",
                "\n",
                "schema = StructType() \\\n",
                "    .add(\"BranchKey\", StringType(), True) \\\n",
                "    .add(\"Branch\", StringType(), True)\n",
                "\n",
                "df = spark.read.format(\"csv\") \\\n",
                "    .option(\"header\",\"true\") \\\n",
                "    .schema(schema) \\\n",
                "    .load(files_path + table_name + \".csv\") \\\n",
                "    .withColumn(\"BranchKey\", lpad(col(\"BranchKey\"), 2, \"0\")) \n",
                "\n",
                "df.write.format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .save(tables_path + table_name)\n",
                "    "
            ],
            "outputs": [],
            "execution_count": 5,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-05T23:24:13.9546618Z\",\"execution_finish_time\":\"2025-06-05T23:24:18.676349Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "be170610-5a6a-4028-b6f3-09239eb208df"
        },
        {
            "cell_type": "code",
            "source": [
                "# FactBudgetActual\n",
                "table_name = \"FactBudgetActual\"\n",
                "url = blob_path + table_name + \".csv\"\n",
                "response = requests.get(url)\n",
                "\n",
                "local_path = f\"/tmp/{table_name}.csv\"\n",
                "with open(local_path, 'wb') as f:\n",
                "    f.write(response.content)\n",
                "\n",
                "mssparkutils.fs.cp(f\"file:{local_path}\", files_path + table_name + \".csv\")\n",
                "\n",
                "schema = StructType() \\\n",
                "    .add(\"BranchKey\", StringType(), True) \\\n",
                "    .add(\"Date\", DateType(), True) \\\n",
                "    .add(\"SubsidiaryAccountKey\", StringType(), True) \\\n",
                "    .add(\"BudgetAmount\", DoubleType(), True) \\\n",
                "    .add(\"ActualAmount\", DoubleType(), True)\n",
                "\n",
                "df = spark.read.format(\"csv\") \\\n",
                "    .option(\"header\",\"true\") \\\n",
                "    .schema(schema) \\\n",
                "    .load(files_path + table_name + \".csv\") \\\n",
                "    .withColumn(\"BranchKey\", lpad(col(\"BranchKey\"), 2, \"0\")) \\\n",
                "    .withColumn(\"SubsidiaryAccountKey\", lpad(col(\"SubsidiaryAccountKey\"), 11, \"0\"))\n",
                "\n",
                "df.write.format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .save(tables_path + table_name)\n"
            ],
            "outputs": [],
            "execution_count": 6,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-05T23:24:18.6787132Z\",\"execution_finish_time\":\"2025-06-05T23:24:23.3480293Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "4a2897e3-7804-4ea8-95d7-ee59233a49f4"
        },
        {
            "cell_type": "code",
            "source": [
                "# DimClient\n",
                "table_name = \"DimClient\"\n",
                "url = blob_path + table_name + \".csv\"\n",
                "response = requests.get(url)\n",
                "\n",
                "local_path = f\"/tmp/{table_name}.csv\"\n",
                "with open(local_path, 'wb') as f:\n",
                "    f.write(response.content)\n",
                "\n",
                "mssparkutils.fs.cp(f\"file:{local_path}\", files_path + table_name + \".csv\")\n",
                "\n",
                "schema = StructType() \\\n",
                "    .add(\"Branch\", StringType(), True) \\\n",
                "    .add(\"Client\", StringType(), True)\n",
                "\n",
                "df = spark.read.format(\"csv\") \\\n",
                "    .option(\"header\",\"true\") \\\n",
                "    .schema(schema) \\\n",
                "    .load(files_path + table_name + \".csv\")\n",
                "\n",
                "df.write.format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .save(tables_path + table_name)\n"
            ],
            "outputs": [],
            "execution_count": 3,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-05T23:47:09.9462748Z\",\"execution_finish_time\":\"2025-06-05T23:47:45.5475711Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "3a0ccf05-a801-4c33-89b2-75ee07e30be3"
        },
        {
            "cell_type": "code",
            "source": [
                "# Measures\n",
                "table_name = \"__Measures\"\n",
                "\n",
                "schema = StructType() \\\n",
                "    .add(\"Value\", StringType(), True) \n",
                "\n",
                "df = spark.createDataFrame(data=[], schema=schema)\n",
                "\n",
                "df.write.format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .save(tables_path + table_name)\n"
            ],
            "outputs": [],
            "execution_count": 8,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "collapsed": false,
                "cellStatus": "{\"Alison Pezzott\":{\"session_start_time\":null,\"execution_start_time\":\"2025-06-05T23:24:26.738048Z\",\"execution_finish_time\":\"2025-06-05T23:24:32.9154808Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "2e314fdc-bd1f-469e-8ce4-5f34b69b43b1"
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}